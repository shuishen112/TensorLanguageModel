{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/wzm289/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae7aaef3f614feab851d0637f92c653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'sentence': ['hide new secretions from the parental units ',\n",
       "  'contains no wit , only labored gags ',\n",
       "  'that loves its characters and communicates something rather beautiful about human nature ',\n",
       "  'remains utterly satisfied to remain the same throughout ',\n",
       "  'on the worst revenge-of-the-nerds clichÃ©s the filmmakers could dredge up ',\n",
       "  \"that 's far too tragic to merit such superficial treatment \",\n",
       "  'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop . ',\n",
       "  'of saucy ',\n",
       "  \"a depressed fifteen-year-old 's suicidal poetry \",\n",
       "  \"are more deeply thought through than in most ` right-thinking ' films \"],\n",
       " 'label': [0, 0, 1, 0, 0, 0, 1, 1, 0, 1],\n",
       " 'idx': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 15696 words in dict\n"
     ]
    }
   ],
   "source": [
    "# preprocessing and tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def get_alphabet(corpuses):\n",
    "\t\"\"\"\n",
    "\tobtain the dict\n",
    "\t\t\t:param corpuses: \n",
    "\t\"\"\"\n",
    "\tword_counter = Counter()\n",
    "\n",
    "\tfor corpus in corpuses:\n",
    "\t\tfor item in corpus:\n",
    "\t\t\ttokens = tokenizer(item['sentence'])\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\tword_counter[token] += 1\n",
    "\tprint(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\t# logging.info(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\tword_dict = {word: e + 2 for e, word in enumerate(list(word_counter))}\n",
    "\tword_dict['UNK'] = 1\n",
    "\tword_dict['<PAD>'] = 0\n",
    "\n",
    "\treturn word_dict\n",
    "\n",
    "vocab = get_alphabet([dataset['train'],dataset['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epch 100000\n",
      "epch 200000\n",
      "epch 300000\n",
      "epch 400000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# get embedding\n",
    "import numpy as np \n",
    "def get_embedding(alphabet, filename=\"\", embedding_size=100):\n",
    "\tembedding = np.random.rand(len(alphabet), embedding_size)\n",
    "\tif filename is None:\n",
    "\t\treturn embedding\n",
    "\twith open(filename, encoding='utf-8') as f:\n",
    "\t\ti = 0\n",
    "\t\tfor line in f:\n",
    "\t\t\ti += 1\n",
    "\t\t\tif i % 100000 == 0:\n",
    "\t\t\t\tprint('epch %d' % i)\n",
    "\t\t\titems = line.strip().split(' ')\n",
    "\t\t\tif len(items) == 2:\n",
    "\t\t\t\tvocab_size, embedding_size = items[0], items[1]\n",
    "\t\t\t\tprint((vocab_size, embedding_size))\n",
    "\t\t\telse:\n",
    "\t\t\t\tword = items[0]\n",
    "\t\t\t\tif word in alphabet:\n",
    "\t\t\t\t\tembedding[alphabet[word]] = items[1:]\n",
    "\n",
    "\tprint('done')\n",
    "\treturn embedding\n",
    "embedding = get_embedding(vocab, filename=\"../embedding/glove.6B.300d.txt\",embedding_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15698, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 111, 78, 470, 0, 0, 0, 0, 0, 0]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# convert to index\n",
    "\n",
    "def convert_to_word_ids(sentence,alphabet,max_len = 40):\n",
    "\t\"\"\"\n",
    "\tdocstring here\n",
    "\t\t:param sentence: \n",
    "\t\t:param alphabet: \n",
    "\t\t:param max_len=40: \n",
    "\t\"\"\"\n",
    "\tindices = []\n",
    "\ttokens = tokenizer(sentence)\n",
    "\tfor word in tokens:\n",
    "\t\tif word in alphabet:\n",
    "\t\t\tindices.append(alphabet[word])\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\tresult = indices + [alphabet['<PAD>']] * (max_len - len(indices))\n",
    "\n",
    "\treturn result[:max_len], min(len(indices),max_len)\n",
    "\n",
    "test_enc, length = convert_to_word_ids(\"hello, how are you\", vocab, 10)\n",
    "print(test_enc)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data batch and iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "class DataMaper(Dataset):\n",
    "    def __init__(self,dataset,vocab):\n",
    "        self.x = dataset['sentence']\n",
    "        self.y = dataset['label']\n",
    "        self.max_length = 120\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        enc_sentence,lengths = convert_to_word_ids(sentence, self.vocab, max_len = self.max_length)\n",
    "        t_sentence = torch.tensor(enc_sentence).to(device)\n",
    "        t_label = torch.tensor(label).to(device)\n",
    "        t_length = torch.tensor(lengths).to(device)\n",
    "        return t_sentence,t_label,t_length\n",
    "\n",
    "train = DataMaper(dataset['train'],vocab)\n",
    "validation = DataMaper(dataset['validation'],vocab)\n",
    "test = DataMaper(dataset['test'], vocab)\n",
    "\n",
    "loader_train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "loader_validation = DataLoader(validation, batch_size = batch_size)\n",
    "loader_test = DataLoader(test,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in loader_train:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(probs, target):\n",
    "    predictions = probs.argmax(dim=1)\n",
    "    corrects = (predictions == target)\n",
    "    accuracy = corrects.sum().float() / float(target.size(0))\n",
    "    return accuracy\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, label,lengths = batch\n",
    "        predictions = model(text,lengths)\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label,lengths = batch\n",
    "            predictions = model(text,lengths)\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = cal_accuracy(predictions, label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type):\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train the model\n",
    "        model.train()\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        # evaluate the model\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        # save the best model\n",
    "        # if valid_loss < best_valid_loss:\n",
    "        #     best_valid_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), 'saved_weights'+'_'+model_type+'.pt')\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my own RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    # you can also accept arguments in your model constructor\n",
    "\n",
    "    #  we don't use the output in this implemention\n",
    "    def __init__(self, embed_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = embed_size + hidden_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(input_size, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "        hidden = torch.sigmoid(self.i2h(input))\n",
    "        output = self.h2o(input)\n",
    "        return output, hidden\n",
    "    def initHidden(self,batch_size):\n",
    "        # return torch.zeros(batch_size,self.hidden_size).to(self.device)\n",
    "        return nn.init.kaiming_uniform_(torch.empty(batch_size, self.hidden_size)).to(self.device)\n",
    "class RNN_layer(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,hidden_dim,output_size):\n",
    "        super(RNN_layer,self).__init__()\n",
    "        self.rnn = RNN(embed_size,hidden_dim,output_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size,padding_idx=0)\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self,x, text_lens):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        x = self.dropout(self.embedding(x))\n",
    "\n",
    "        hidden = self.rnn.initHidden(batch_size)\n",
    "        hiddens = []\n",
    "        # recurrent rnn\n",
    "        for i in range(seq_len):\n",
    "            output, hidden_next = self.rnn(x[:,i,:], hidden)\n",
    "            mask = (i < text_lens).float().unsqueeze(1).expand_as(hidden_next).to(device)\n",
    "            # if hidden_next is \n",
    "            hidden_next = (hidden_next * mask + hidden * (1 - mask)).to(device)\n",
    "            hiddens.append(hidden_next.unsqueeze(1))\n",
    "            hidden = hidden_next\n",
    "        final_hidden = hidden\n",
    "        hidden_tensor = torch.cat(hiddens,1)\n",
    "        return hidden_tensor,final_hidden,output\n",
    "        \n",
    "\n",
    "class RNN_Model_for_classfication(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,hidden_dim,output_size):\n",
    "        super(RNN_Model_for_classfication,self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.vocab_size = vocab_size \n",
    "        # define the layer\n",
    "        # self.rnn = nn.RNN(embed_size,hidden_dim,num_layers = 1,batch_first= True)\n",
    "        self.rnn_layer = RNN_layer(self.vocab_size,embed_size,hidden_dim,output_size)\n",
    "        self.fc = nn.Linear(hidden_dim,output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self,x,lens):\n",
    "    \n",
    "        hidden_tensor, final_hidden , output = self.rnn_layer(x, lens)\n",
    "\n",
    "        out = output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000015vscode-remote?line=6'>7</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(rnn_model\u001b[39m.\u001b[39mparameters(),lr \u001b[39m=\u001b[39m lr)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000015vscode-remote?line=7'>8</a>\u001b[0m loss_func \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000015vscode-remote?line=8'>9</a>\u001b[0m run_train(epochs,rnn_model,loader_train,loader_validation,optimizer,loss_func,model_type \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb Cell 13'\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=7'>8</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=10'>11</a>\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, label)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=15'>16</a>\u001b[0m acc \u001b[39m=\u001b[39m cal_accuracy(predictions, label)        \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/TensorLanguageModel/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=18'>19</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "rnn_model = RNN_Model_for_classfication(vocab_size = len(vocab),embed_size = 300,hidden_dim = 256,\n",
    "output_size = 2)\n",
    "rnn_model.to(device)\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(),lr = lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "run_train(epochs,rnn_model,loader_train,loader_validation,optimizer,loss_func,model_type = \"cls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TN(nn.Module):\n",
    "\n",
    "    # tensor network unit\n",
    "    def __init__(self, rank, output_size):\n",
    "        super(TN, self).__init__()\n",
    "\n",
    "        self.rank = rank\n",
    "        self.output_size = output_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.device = torch.device('cpu')\n",
    "        self.i2h = nn.Linear(self.rank, self.rank)\n",
    "        self.x2w = nn.Linear(1, self.rank)\n",
    "        self.h2o = nn.Linear(self.rank, output_size)\n",
    "    \n",
    "\n",
    "    def forward(self, data, m):\n",
    "        # input = torch.cat((data, m.squeeze(1)), 1)\n",
    "\n",
    "        # hidden = self.i2h(input)\n",
    "        # output = self.h2o(hidden)\n",
    "\n",
    "        # unit = self.i2h(data)\n",
    "        unit = data.contiguous().view(-1,self.rank,self.rank)\n",
    "        # get hidden\n",
    "        activition = torch.nn.Tanh()\n",
    "        # batch_size = unit.size(0)\n",
    "\n",
    "        # weight = self.i2h.weight.unsqueeze(0).repeat([batch_size,1,1])\n",
    "        # unit = torch.einsum(\"bij,bjk->bik\",[unit,weight])\n",
    "        m = activition(torch.einsum(\"bij,bjk->bik\",[m,unit]))\n",
    "        \n",
    "        # # m = unit\n",
    "        hidden = self.i2h(m)\n",
    "        output = self.h2o(hidden)\n",
    "        return hidden, output\n",
    "\n",
    "    def init_m1(self,batch_size):\n",
    "        return torch.ones(batch_size,1,self.rank).to(self.device)\n",
    "        # return nn.Linear(1,self.rank).to(self.device)\n",
    "    def init_m2(self):\n",
    "        return nn.Linear(self.rank, self.output_size)\n",
    "    def init_hidden(self,batch_size):\n",
    "        return torch.zeros(batch_size,self.rank).to(self.device)\n",
    "\n",
    "class TN_layer(nn.Module):\n",
    "    def __init__(self,rank,vocab_size,output_size):\n",
    "        super(TN_layer,self).__init__()\n",
    "        self.tn = TN(rank,output_size)\n",
    "        self.rank = rank\n",
    "        self.embedding = nn.Embedding(vocab_size,self.rank * self.rank,padding_idx=0)\n",
    "\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        encoding = self.embedding(x)\n",
    "        \n",
    "        # m = self.tn.init_hidden(batch_size)\n",
    "        m = self.tn.init_m1(batch_size)\n",
    "        # m = m.weight.view(-1,self.rank).unsqueeze(0).repeat([batch_size,1,1])\n",
    "        hiddens = []\n",
    "        # recurrent tn\n",
    "        for i in range(seq_len):\n",
    "            m, output = self.tn(encoding[:,i,:], m)\n",
    "            hiddens.append(m)\n",
    "        final_hidden = m\n",
    "        hidden_tensor = torch.cat(hiddens,1)\n",
    "        return hidden_tensor,final_hidden,output\n",
    "        \n",
    "\n",
    "class TN_model_for_classfication(nn.Module):\n",
    "    def __init__(self,rank,vocab_size, output_size):\n",
    "        super(TN_model_for_classfication,self).__init__()\n",
    "\n",
    "        self.rank = rank\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.tn = TN_layer(self.rank, self.vocab_size, output_size)\n",
    "        self.fc = nn.Linear(self.rank,output_size)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self,x, lens):\n",
    "        seq_output, hidden, output = self.tn(x)\n",
    "        # out = out.contiguous().view(-1,self.rank)\n",
    "        # output = self.fc(hidden.squeeze(1))\n",
    "        output = output.squeeze(1)\n",
    "\n",
    "        \n",
    "        return output\n",
    "    def init_hidden(self,batch_size):\n",
    "        hidden = torch.zeros(self.n_layers,batch_size,self.hidden_dim).to(self.device)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000023vscode-remote?line=5'>6</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(tn_model\u001b[39m.\u001b[39mparameters(),lr \u001b[39m=\u001b[39m lr)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000023vscode-remote?line=6'>7</a>\u001b[0m loss_func \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000023vscode-remote?line=7'>8</a>\u001b[0m run_train(epochs,tn_model,loader_train,loader_validation,optimizer,loss_func,model_type \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb Cell 13'\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=7'>8</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000012vscode-remote?line=10'>11</a>\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb Cell 12'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=16'>17</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=17'>18</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=18'>19</a>\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=19'>20</a>\u001b[0m     epoch_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m acc\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/RNN_one_batch.ipynb#ch0000011vscode-remote?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m epoch_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(iterator), epoch_acc \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(iterator)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "tn_model = TN_model_for_classfication(rank = 256, vocab_size = len(vocab),output_size = 2)\n",
    "tn_model.to(device)\n",
    "optimizer = torch.optim.Adam(tn_model.parameters(),lr = lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "run_train(epochs,tn_model,loader_train,loader_validation,optimizer,loss_func,model_type = \"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
