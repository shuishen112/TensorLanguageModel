{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch import nn \n",
    "import torch \n",
    "\n",
    "\n",
    "\n",
    "def cal_accuracy(probs, target):\n",
    "    predictions = probs.argmax(dim=1)\n",
    "    corrects = predictions == target\n",
    "    accuracy = corrects.sum().float() / float(target.size(0))\n",
    "    return accuracy\n",
    "class TN(nn.Module):\n",
    "\n",
    "    # tensor network unit\n",
    "    def __init__(self, rank, output_size):\n",
    "        super(TN, self).__init__()\n",
    "\n",
    "        self.rank = rank\n",
    "        self.output_size = output_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # self.device = torch.device('cpu')\n",
    "        self.i2h = nn.Linear(self.rank, self.rank)\n",
    "        self.h2o = nn.Linear(self.rank, output_size)\n",
    "\n",
    "    def forward(self, data, m):\n",
    "        # input = torch.cat((data, m.squeeze(1)), 1)\n",
    "\n",
    "        # hidden = self.i2h(input)\n",
    "        # output = self.h2o(hidden)\n",
    "\n",
    "        # unit = self.i2h(data)\n",
    "        unit = data.contiguous().view(-1, self.rank, self.rank)\n",
    "        # get hidden\n",
    "        activition = torch.nn.Tanh()\n",
    "        # batch_size = unit.size(0)\n",
    "\n",
    "        # weight = self.i2h.weight.unsqueeze(0).repeat([batch_size,1,1])\n",
    "        # unit = torch.einsum(\"bij,bjk->bik\",[unit,weight])\n",
    "        m = activition(torch.einsum(\"bij,bjk->bik\", [m, unit]))\n",
    "\n",
    "        # # m = unit\n",
    "        hidden = self.i2h(m)\n",
    "        output = self.h2o(hidden)\n",
    "        return hidden, output\n",
    "\n",
    "    def init_m1(self, batch_size):\n",
    "        return torch.ones(batch_size, 1, self.rank).to(self.device)\n",
    "        # return nn.Linear(1,self.rank).to(self.device)\n",
    "\n",
    "    def init_m2(self):\n",
    "        return nn.Linear(self.rank, self.output_size)\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.rank).to(self.device)\n",
    "\n",
    "\n",
    "class TN_layer(nn.Module):\n",
    "    def __init__(self, rank, vocab_size, output_size):\n",
    "        super(TN_layer, self).__init__()\n",
    "        self.tn = TN(rank, output_size)\n",
    "        self.rank = rank\n",
    "        self.embedding = nn.Embedding(vocab_size, self.rank * self.rank, padding_idx=0)\n",
    "\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        encoding = self.embedding(x)\n",
    "\n",
    "        # m = self.tn.init_hidden(batch_size)\n",
    "        m = self.tn.init_m1(batch_size)\n",
    "        # m = m.weight.view(-1,self.rank).unsqueeze(0).repeat([batch_size,1,1])\n",
    "        hiddens = []\n",
    "        # recurrent tn\n",
    "        for i in range(seq_len):\n",
    "            m, output = self.tn(encoding[:, i, :], m)\n",
    "            hiddens.append(m)\n",
    "        final_hidden = m\n",
    "        hidden_tensor = torch.cat(hiddens, 1)\n",
    "        return hidden_tensor, final_hidden\n",
    "\n",
    "\n",
    "class TN_model_for_classfication(nn.Module):\n",
    "    def __init__(self, rank, vocab_size, output_size):\n",
    "        super(TN_model_for_classfication, self).__init__()\n",
    "\n",
    "        self.rank = rank\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.tn = TN_layer(self.rank, self.vocab_size, output_size)\n",
    "        self.fc = nn.Linear(self.rank, output_size)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        seq_output, hidden = self.tn(x)\n",
    "        # out = out.contiguous().view(-1,self.rank)\n",
    "        output = self.fc(hidden.squeeze(1))\n",
    "\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n",
    "        return hidden\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(256, 512), nn.BatchNorm1d(512))\n",
    "        self.example_input_array = torch.zeros(10, 256)  # optional\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Baselignthing(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        encode = self.model(x, lengths)\n",
    "        return encode\n",
    "\n",
    "    # optimizers go into configure_optimizer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "    # train and validation\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        text, label, lengths = train_batch\n",
    "        predictions = self.model(text, lengths)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"acc\", acc)\n",
    "        return {\"loss\": loss, \"train_acc\": acc}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        text, label, lengths = val_batch\n",
    "        predictions = self.model(text, lengths)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        return acc\n",
    "\n",
    "    def training_epoch_end(self, train_step_outputs) -> None:\n",
    "        all_acc = torch.stack([x[\"train_acc\"] for x in train_step_outputs])\n",
    "        print(\"train_epoch_acc:\", torch.mean(all_acc))\n",
    "        self.log(\"train_epoch_acc\", torch.mean(all_acc))\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        all_acc = torch.stack(validation_step_outputs)\n",
    "        print(\"val_epoch_acc:\", torch.mean(all_acc))\n",
    "        self.log(\"val_epoch_acc\", torch.mean(all_acc))\n",
    "\n",
    "class litTNLM(Baselignthing):\n",
    "    def __init__(self, rank, vocab_size, output_size):\n",
    "        super().__init__()\n",
    "        self.model = TN_model_for_classfication(\n",
    "            rank=rank, vocab_size=vocab_size, output_size=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 10000\n",
    "model = litTNLM(rank=100, vocab_size=vocab, output_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  | Name               | Type                       | Params\n",
       "------------------------------------------------------------------\n",
       "0 | model              | TN_model_for_classfication | 100 M \n",
       "1 | model.tn           | TN_layer                   | 100 M \n",
       "2 | model.tn.tn        | TN                         | 10.3 K\n",
       "3 | model.tn.tn.i2h    | Linear                     | 10.1 K\n",
       "4 | model.tn.tn.h2o    | Linear                     | 202   \n",
       "5 | model.tn.embedding | Embedding                  | 100 M \n",
       "6 | model.tn.dropout   | Dropout                    | 0     \n",
       "7 | model.fc           | Linear                     | 202   \n",
       "8 | model.softmax      | Softmax                    | 0     \n",
       "9 | model.log_softmax  | LogSoftmax                 | 0     \n",
       "------------------------------------------------------------------\n",
       "100 M     Trainable params\n",
       "0         Non-trainable params\n",
       "100 M     Total params\n",
       "400.042   Total estimated model params size (MB)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "ModelSummary(model, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, e_dim, h_dim, o_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_dim, e_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv1 = nn.Conv2d(1, h_dim, (3, e_dim))\n",
    "        self.conv2 = nn.Conv2d(1, h_dim, (4, e_dim))\n",
    "        self.conv3 = nn.Conv2d(1, h_dim, (5, e_dim))\n",
    "        self.fc = nn.Linear(h_dim * 3, o_dim)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        # self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.dropout(self.emb(x)).unsqueeze(1)\n",
    "        c1 = torch.relu(self.conv1(embed).squeeze(3))\n",
    "        p1 = torch.max_pool1d(c1, c1.size()[2]).squeeze(2)\n",
    "        c2 = torch.relu(self.conv2(embed).squeeze(3))\n",
    "        p2 = torch.max_pool1d(c2, c2.size()[2]).squeeze(2)\n",
    "        c3 = torch.relu(self.conv3(embed).squeeze(3))\n",
    "        p3 = torch.max_pool1d(c3, c3.size()[2]).squeeze(2)\n",
    "        pool = self.dropout(torch.cat((p1, p2, p3), 1))\n",
    "        hidden = self.fc(pool)\n",
    "        # return self.softmax(hidden), self.log_softmax(hidden)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "class litCNN(Baselignthing):\n",
    "    def __init__(self, vocab_dim, e_dim, h_dim, o_dim):\n",
    "        super().__init__()\n",
    "        self.model = CNN(vocab_dim, e_dim, h_dim, o_dim)\n",
    "\n",
    "        # find the batch_size\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode = self.model(x)\n",
    "        return encode\n",
    "\n",
    "    # train and validation\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        text, label, lengths = train_batch\n",
    "        predictions = self.model(text)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.log(\"acc\", acc)\n",
    "        return {\"loss\": loss, \"train_acc\": acc}\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        text, label, lengths = val_batch\n",
    "        predictions = self.model(text)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "litCNN(\n",
      "  (model): CNN(\n",
      "    (emb): Embedding(10000, 300, padding_idx=0)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (conv2): Conv2d(1, 64, kernel_size=(4, 300), stride=(1, 1))\n",
      "    (conv3): Conv2d(1, 64, kernel_size=(5, 300), stride=(1, 1))\n",
      "    (fc): Linear(in_features=192, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = litCNN(10000,e_dim = 300,h_dim = 64, o_dim = 2)\n",
    "print(model)\n",
    "# ModelSummary(model, max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    # you can also accept arguments in your model constructor\n",
    "\n",
    "    #  we don't use the output in this implemention\n",
    "    def __init__(self, embed_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = embed_size + hidden_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.Wih = nn.Parameter(torch.FloatTensor(embed_size,hidden_size))\n",
    "        self.Whh = nn.Parameter(torch.FloatTensor(hidden_size,hidden_size))\n",
    "        # self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "        # hidden = torch.sigmoid(self.i2h(input))\n",
    "        wi = torch.mm(data,self.Wih)\n",
    "        wh = torch.mm(last_hidden,self.Whh)\n",
    "\n",
    "\n",
    "        hidden = torch.tanh(wi + wh)\n",
    "        output = self.h2o(input)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        # return torch.zeros(batch_size,self.hidden_size).to(self.device)\n",
    "        return nn.init.kaiming_uniform_(torch.empty(batch_size, self.hidden_size)).to(\n",
    "            self.device\n",
    "        )\n",
    "\n",
    "\n",
    "class RNN_layer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_dim, output_size):\n",
    "        super(RNN_layer, self).__init__()\n",
    "        self.rnn = RNN(embed_size, hidden_dim, output_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        x = self.dropout(self.embedding(x))\n",
    "\n",
    "        hidden = self.rnn.initHidden(batch_size)\n",
    "        hiddens = []\n",
    "        # recurrent rnn\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = self.rnn(x[:, i, :], hidden)\n",
    "            hiddens.append(hidden.unsqueeze(1))\n",
    "        final_hidden = hidden\n",
    "        hidden_tensor = torch.cat(hiddens, 1)\n",
    "        return hidden_tensor, final_hidden, output\n",
    "\n",
    "\n",
    "class RNN_Model_for_classfication(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_dim, output_size):\n",
    "        super(RNN_Model_for_classfication, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        # define the layer\n",
    "        # self.rnn = nn.RNN(embed_size,hidden_dim,num_layers = 1,batch_first= True)\n",
    "        self.rnn = RNN_layer(self.vocab_size, embed_size, hidden_dim, output_size)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "\n",
    "        hidden_tensor, final_hidden, output = self.rnn(x)\n",
    "\n",
    "        out = self.fc(final_hidden)\n",
    "        return out\n",
    "\n",
    "\n",
    "class litSimpleRNN(Baselignthing):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_dim, output_size):\n",
    "        super().__init__()\n",
    "        self.model = RNN_Model_for_classfication(\n",
    "            vocab_size, embed_size, hidden_dim, output_size\n",
    "        )\n",
    "\n",
    "        # find the batch_size\n",
    "        self.save_hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.rnn.rnn.Wih\n",
      "model.rnn.rnn.Whh\n",
      "model.rnn.rnn.h2o.weight\n",
      "model.rnn.rnn.h2o.bias\n",
      "model.rnn.embedding.weight\n",
      "model.fc.weight\n",
      "model.fc.bias\n"
     ]
    }
   ],
   "source": [
    "model = litSimpleRNN(\n",
    "    vocab_size=10000, embed_size=300, hidden_dim=256, output_size=2\n",
    ")\n",
    "for name, _ in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
