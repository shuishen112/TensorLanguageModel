{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/wzm289/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7d9e6a4550481cbfe23b8c2d186f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "task_data = \"sst2\"\n",
    "if task_data == \"sst2\":\n",
    "    dataset = load_dataset(\"glue\", \"sst2\")\n",
    "else:\n",
    "    dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(map(lambda x:len(x.split()),dataset['train']['sentence']))\n",
    "# print(max(map(lambda x:len(x.split()),dataset['train']['sentence1'])))\n",
    "# print(max(map(lambda x:len(x.split()),dataset['train']['sentence2'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 15696 words in dict\n"
     ]
    }
   ],
   "source": [
    "# preprocessing and tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def get_alphabet(corpuses):\n",
    "\t\"\"\"\n",
    "\tobtain the dict\n",
    "\t\t\t:param corpuses: \n",
    "\t\"\"\"\n",
    "\tword_counter = Counter()\n",
    "\n",
    "\tfor corpus in corpuses:\n",
    "\t\tfor item in corpus:\n",
    "\t\t\tif task_data == \"mrpc\":\n",
    "\t\t\t\ttokens = tokenizer(item['sentence1'] + ' ' + item['sentence1'])\n",
    "\t\t\telse:\n",
    "\t\t\t\ttokens = tokenizer(item['sentence'])\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\tword_counter[token] += 1\n",
    "\tprint(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\t# logging.info(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\tword_dict = {word: e + 2 for e, word in enumerate(list(word_counter))}\n",
    "\tword_dict['UNK'] = 1\n",
    "\tword_dict['<PAD>'] = 0\n",
    "\n",
    "\treturn word_dict\n",
    "\n",
    "vocab = get_alphabet([dataset['train'],dataset['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epch 100000\n",
      "epch 200000\n",
      "epch 300000\n",
      "epch 400000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# get embedding\n",
    "import numpy as np \n",
    "def get_embedding(alphabet, filename=\"\", embedding_size=100):\n",
    "\tembedding = np.random.rand(len(alphabet), embedding_size)\n",
    "\tif filename is None:\n",
    "\t\treturn embedding\n",
    "\twith open(filename, encoding='utf-8') as f:\n",
    "\t\ti = 0\n",
    "\t\tfor line in f:\n",
    "\t\t\ti += 1\n",
    "\t\t\tif i % 100000 == 0:\n",
    "\t\t\t\tprint('epch %d' % i)\n",
    "\t\t\titems = line.strip().split(' ')\n",
    "\t\t\tif len(items) == 2:\n",
    "\t\t\t\tvocab_size, embedding_size = items[0], items[1]\n",
    "\t\t\t\tprint((vocab_size, embedding_size))\n",
    "\t\t\telse:\n",
    "\t\t\t\tword = items[0]\n",
    "\t\t\t\tif word in alphabet:\n",
    "\t\t\t\t\tembedding[alphabet[word]] = items[1:]\n",
    "\n",
    "\tprint('done')\n",
    "\treturn embedding\n",
    "embedding = get_embedding(vocab, filename=\"../embedding/glove.6B.300d.txt\",embedding_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15698, 300)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 111, 78, 470, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to index\n",
    "\n",
    "def convert_to_word_ids(sentence,alphabet,max_len = 40):\n",
    "\t\"\"\"\n",
    "\tdocstring here\n",
    "\t\t:param sentence: \n",
    "\t\t:param alphabet: \n",
    "\t\t:param max_len=40: \n",
    "\t\"\"\"\n",
    "\tindices = []\n",
    "\ttokens = tokenizer(sentence)\n",
    "\t\n",
    "\tfor word in tokens:\n",
    "\t\tif word in alphabet:\n",
    "\t\t\tindices.append(alphabet[word])\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\tresult = indices + [alphabet['<PAD>']] * (max_len - len(indices))\n",
    "\n",
    "\treturn result[:max_len]\n",
    "\n",
    "test_enc = convert_to_word_ids(\"hello, how are you\", vocab, 10)\n",
    "test_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data batch and iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "max_document_length = 200\n",
    "class DataMaper(Dataset):\n",
    "    def __init__(self,dataset,vocab,max_document_length):\n",
    "        if task_data == 'sst2':\n",
    "            self.x = dataset['sentence']\n",
    "        else:\n",
    "            self.x1 = dataset['sentence1'] \n",
    "            self.x2 = dataset['sentence2'] \n",
    "        self.y = dataset['label']\n",
    "        self.max_length = max_document_length\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if task_data == 'mrpc':\n",
    "            sentence = self.x1[idx] + ' ' + self.x2[idx]\n",
    "        else:\n",
    "            sentence = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        enc_sentence = convert_to_word_ids(sentence, self.vocab, max_len = self.max_length)\n",
    "        t_sentence = torch.tensor(enc_sentence).to(device)\n",
    "        t_label = torch.tensor(label).to(device)\n",
    "        return t_sentence,t_label\n",
    "\n",
    "train = DataMaper(dataset['train'],vocab,max_document_length = 200)\n",
    "validation = DataMaper(dataset['validation'],vocab,max_document_length = 200)\n",
    "test = DataMaper(dataset['test'], vocab,max_document_length = 200)\n",
    "\n",
    "loader_train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "loader_validation = DataLoader(validation, batch_size = batch_size)\n",
    "loader_test = DataLoader(test,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in loader_train:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size, num_class):\n",
    "        super(Linear, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size,hidden_size, bias = True)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_class,bias = True)\n",
    "\n",
    "    def forward(self, text):\n",
    "        text = text.float()\n",
    "        x = self.fc1(text)\n",
    "        preds = self.fc2(x)\n",
    "        return preds\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(probs, target):\n",
    "    predictions = probs.argmax(dim=1)\n",
    "    corrects = (predictions == target)\n",
    "    accuracy = corrects.sum().float() / float(target.size(0))\n",
    "    return accuracy\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, label = batch\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, label.squeeze())\n",
    "        acc = cal_accuracy(predictions, label)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label = batch\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = cal_accuracy(predictions, label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type):\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train the model\n",
    "        model.train()\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        # evaluate the model\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        # save the best model\n",
    "        # if valid_loss < best_valid_loss:\n",
    "        #     best_valid_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), 'saved_weights'+'_'+model_type+'.pt')\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 7.086 | Train Acc: 51.23%\n",
      "\t Val. Loss: 3.785 |  Val. Acc: 51.29%\n",
      "\tTrain Loss: 1.444 | Train Acc: 52.41%\n",
      "\t Val. Loss: 3.833 |  Val. Acc: 49.42%\n",
      "\tTrain Loss: 1.384 | Train Acc: 52.46%\n",
      "\t Val. Loss: 3.418 |  Val. Acc: 49.26%\n",
      "\tTrain Loss: 1.405 | Train Acc: 53.06%\n",
      "\t Val. Loss: 3.631 |  Val. Acc: 48.93%\n",
      "\tTrain Loss: 1.450 | Train Acc: 53.02%\n",
      "\t Val. Loss: 3.695 |  Val. Acc: 48.86%\n",
      "\tTrain Loss: 1.367 | Train Acc: 52.72%\n",
      "\t Val. Loss: 3.616 |  Val. Acc: 50.09%\n",
      "\tTrain Loss: 1.365 | Train Acc: 53.19%\n",
      "\t Val. Loss: 4.431 |  Val. Acc: 51.99%\n",
      "\tTrain Loss: 1.351 | Train Acc: 53.25%\n",
      "\t Val. Loss: 2.863 |  Val. Acc: 50.16%\n",
      "\tTrain Loss: 1.356 | Train Acc: 53.20%\n",
      "\t Val. Loss: 3.500 |  Val. Acc: 50.16%\n",
      "\tTrain Loss: 1.332 | Train Acc: 53.28%\n",
      "\t Val. Loss: 3.518 |  Val. Acc: 48.62%\n",
      "\tTrain Loss: 1.400 | Train Acc: 53.21%\n",
      "\t Val. Loss: 3.239 |  Val. Acc: 50.25%\n",
      "\tTrain Loss: 1.305 | Train Acc: 53.23%\n",
      "\t Val. Loss: 3.696 |  Val. Acc: 49.78%\n",
      "\tTrain Loss: 1.325 | Train Acc: 53.61%\n",
      "\t Val. Loss: 3.966 |  Val. Acc: 50.98%\n",
      "\tTrain Loss: 1.326 | Train Acc: 53.50%\n",
      "\t Val. Loss: 2.898 |  Val. Acc: 50.07%\n",
      "\tTrain Loss: 1.319 | Train Acc: 53.30%\n",
      "\t Val. Loss: 3.796 |  Val. Acc: 50.36%\n",
      "\tTrain Loss: 1.338 | Train Acc: 53.00%\n",
      "\t Val. Loss: 2.587 |  Val. Acc: 46.25%\n",
      "\tTrain Loss: 1.249 | Train Acc: 53.50%\n",
      "\t Val. Loss: 4.043 |  Val. Acc: 50.92%\n",
      "\tTrain Loss: 1.317 | Train Acc: 52.91%\n",
      "\t Val. Loss: 3.404 |  Val. Acc: 50.96%\n",
      "\tTrain Loss: 1.271 | Train Acc: 53.42%\n",
      "\t Val. Loss: 2.838 |  Val. Acc: 50.56%\n",
      "\tTrain Loss: 1.279 | Train Acc: 53.32%\n",
      "\t Val. Loss: 2.763 |  Val. Acc: 48.59%\n",
      "\tTrain Loss: 1.302 | Train Acc: 53.55%\n",
      "\t Val. Loss: 2.543 |  Val. Acc: 51.72%\n",
      "\tTrain Loss: 1.238 | Train Acc: 53.54%\n",
      "\t Val. Loss: 2.145 |  Val. Acc: 49.46%\n",
      "\tTrain Loss: 1.253 | Train Acc: 53.64%\n",
      "\t Val. Loss: 4.094 |  Val. Acc: 47.52%\n",
      "\tTrain Loss: 1.257 | Train Acc: 53.29%\n",
      "\t Val. Loss: 2.768 |  Val. Acc: 48.30%\n",
      "\tTrain Loss: 1.238 | Train Acc: 53.33%\n",
      "\t Val. Loss: 3.801 |  Val. Acc: 45.87%\n",
      "\tTrain Loss: 1.249 | Train Acc: 53.30%\n",
      "\t Val. Loss: 2.632 |  Val. Acc: 52.05%\n",
      "\tTrain Loss: 1.220 | Train Acc: 53.47%\n",
      "\t Val. Loss: 2.639 |  Val. Acc: 49.24%\n",
      "\tTrain Loss: 1.227 | Train Acc: 53.39%\n",
      "\t Val. Loss: 2.903 |  Val. Acc: 50.69%\n",
      "\tTrain Loss: 1.209 | Train Acc: 53.12%\n",
      "\t Val. Loss: 2.760 |  Val. Acc: 46.99%\n",
      "\tTrain Loss: 1.193 | Train Acc: 53.54%\n",
      "\t Val. Loss: 3.125 |  Val. Acc: 46.09%\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "hidden_size = 50\n",
    "num_classes = 2\n",
    "lr = 1e-4\n",
    "linear_model = Linear(max_document_length,hidden_size,num_classes)\n",
    "linear_model.to(device)\n",
    "optimizer = torch.optim.Adam(linear_model.parameters(),lr = lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "run_train(epochs,linear_model,loader_train,loader_validation,optimizer,loss_func,model_type = \"cls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, e_dim, h_dim, o_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_dim, e_dim, padding_idx=0)\n",
    "        # self.emb.load_state_dict({\"weight\":torch.tensor(embedding)})\n",
    "        non_trainable = True\n",
    "        if non_trainable:\n",
    "            self.emb.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv1 = nn.Conv2d(1, h_dim, (3, e_dim))\n",
    "        self.conv2 = nn.Conv2d(1, h_dim, (4, e_dim))\n",
    "        self.conv3 = nn.Conv2d(1, h_dim, (5, e_dim))\n",
    "        self.fc = nn.Linear(h_dim * 3, o_dim)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        # self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.dropout(self.emb(x)).unsqueeze(1)\n",
    "        c1 = torch.relu(self.conv1(embed).squeeze(3))\n",
    "        p1 = torch.max_pool1d(c1, c1.size()[2]).squeeze(2)\n",
    "        c2 = torch.relu(self.conv2(embed).squeeze(3))\n",
    "        p2 = torch.max_pool1d(c2, c2.size()[2]).squeeze(2)\n",
    "        c3 = torch.relu(self.conv3(embed).squeeze(3))\n",
    "        p3 = torch.max_pool1d(c3, c3.size()[2]).squeeze(2)\n",
    "        pool = self.dropout(torch.cat((p1, p2, p3), 1))\n",
    "        hidden = self.fc(pool)\n",
    "        # return self.softmax(hidden), self.log_softmax(hidden)\n",
    "        return hidden \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 0.626 | Train Acc: 63.79%\n",
      "\t Val. Loss: 0.578 |  Val. Acc: 71.41%\n",
      "\tTrain Loss: 0.521 | Train Acc: 74.27%\n",
      "\t Val. Loss: 0.532 |  Val. Acc: 73.64%\n",
      "\tTrain Loss: 0.445 | Train Acc: 79.56%\n",
      "\t Val. Loss: 0.510 |  Val. Acc: 74.46%\n",
      "\tTrain Loss: 0.388 | Train Acc: 82.86%\n",
      "\t Val. Loss: 0.498 |  Val. Acc: 75.25%\n",
      "\tTrain Loss: 0.346 | Train Acc: 85.10%\n",
      "\t Val. Loss: 0.493 |  Val. Acc: 75.56%\n",
      "\tTrain Loss: 0.316 | Train Acc: 86.60%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 75.67%\n",
      "\tTrain Loss: 0.292 | Train Acc: 87.84%\n",
      "\t Val. Loss: 0.500 |  Val. Acc: 75.49%\n",
      "\tTrain Loss: 0.275 | Train Acc: 88.43%\n",
      "\t Val. Loss: 0.496 |  Val. Acc: 76.21%\n",
      "\tTrain Loss: 0.260 | Train Acc: 89.29%\n",
      "\t Val. Loss: 0.508 |  Val. Acc: 76.38%\n",
      "\tTrain Loss: 0.248 | Train Acc: 89.86%\n",
      "\t Val. Loss: 0.502 |  Val. Acc: 76.50%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "max_document_length = 40\n",
    "hidden_size = 50\n",
    "num_classes = 2\n",
    "lr = 1e-4\n",
    "\n",
    "cnn_model = CNN(len(vocab),e_dim = 300,h_dim = 64, o_dim = 2)\n",
    "cnn_model.to(device)\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(),lr = lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "run_train(epochs,cnn_model,loader_train,loader_validation,optimizer,loss_func,model_type = \"cls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.0260, -0.0000,  0.8843,  ..., -0.2663, -0.6355,  1.4080],\n",
      "         [ 0.0094,  0.0000,  0.2679,  ...,  1.8438,  0.4206,  1.2296],\n",
      "         [ 0.2029, -0.8962,  2.1734,  ...,  0.3732, -0.0762,  0.9980],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.9881,  0.0310,  1.4007,  ..., -0.0000,  0.0000, -1.4308],\n",
      "         [-1.5299,  0.0000, -0.8905,  ..., -0.5306, -2.1391,  0.1005],\n",
      "         [ 2.4643,  1.2164,  0.0000,  ...,  0.5484,  1.5321,  0.7308],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.2813, -1.8355, -1.4775,  ..., -0.0000,  0.0000,  1.2485],\n",
      "         [-0.4552,  0.9911, -1.6375,  ..., -0.0000,  2.3257, -0.0000],\n",
      "         [ 0.5720,  0.2655, -0.6618,  ..., -0.8239,  0.0000, -1.7540],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.7368,  0.7498, -0.7836,  ..., -0.4396, -0.8285, -1.2411],\n",
      "         [-0.3589,  0.6868,  1.2739,  ...,  0.2918,  0.2192,  0.0000],\n",
      "         [-0.0000, -0.5182,  0.4598,  ..., -0.8946,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000, -1.0925, -0.6056,  ...,  0.0000, -0.9621, -1.3545],\n",
      "         [-2.5111,  0.0581, -0.0000,  ..., -3.1388, -1.8397, -1.6851],\n",
      "         [ 2.5018, -0.3250, -1.0231,  ..., -0.0000, -1.7904,  0.8022],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.1324, -0.0000,  ...,  0.4531,  0.5224, -0.0000],\n",
      "         [ 0.0000, -0.9176, -1.2714,  ...,  0.0711,  0.0725, -0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       device='cuda:0', grad_fn=<NativeDropoutBackward0>)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_lengths' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=31'>32</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(rnn_model\u001b[39m.\u001b[39mparameters(),lr \u001b[39m=\u001b[39m lr)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=32'>33</a>\u001b[0m loss_func \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=33'>34</a>\u001b[0m run_train(epochs,rnn_model,loader_train,loader_validation,optimizer,loss_func,model_type \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb Cell 16'\u001b[0m in \u001b[0;36mrun_train\u001b[0;34m(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000015vscode-remote?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000015vscode-remote?line=4'>5</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000015vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m# train the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000015vscode-remote?line=6'>7</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000015vscode-remote?line=7'>8</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, criterion)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000015vscode-remote?line=9'>10</a>\u001b[0m     \u001b[39m# evaluate the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000015vscode-remote?line=10'>11</a>\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion)\n",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb Cell 15'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000014vscode-remote?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000014vscode-remote?line=12'>13</a>\u001b[0m text, label \u001b[39m=\u001b[39m batch\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000014vscode-remote?line=13'>14</a>\u001b[0m predictions \u001b[39m=\u001b[39m model(text)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000014vscode-remote?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(predictions, label\u001b[39m.\u001b[39msqueeze())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000014vscode-remote?line=15'>16</a>\u001b[0m acc \u001b[39m=\u001b[39m cal_accuracy(predictions, label)        \n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb Cell 22'\u001b[0m in \u001b[0;36mRNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=21'>22</a>\u001b[0m embed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(x))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(embed)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=23'>24</a>\u001b[0m packed_embedded \u001b[39m=\u001b[39m pack_padded_sequence(embed, text_lengths, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=25'>26</a>\u001b[0m out, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(embed)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/CNN.ipynb#ch0000021vscode-remote?line=26'>27</a>\u001b[0m hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc(out[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_lengths' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "hidden_size = 50\n",
    "num_classes = 2\n",
    "lr = 1e-3\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, e_dim, h_dim, o_dim):\n",
    "        super(RNN, self).__init__()\n",
    "        self.h_dim = h_dim\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.emb = nn.Embedding(vocab_dim, e_dim, padding_idx=0)\n",
    "        # self.emb.load_state_dict({\"weight\":torch.tensor(embedding)})\n",
    "        # non_trainable = True\n",
    "        # if non_trainable:\n",
    "        #     self.emb.weight.requires_grad = False\n",
    "        self.lstm = nn.RNN(e_dim, h_dim, bidirectional=False, batch_first=True)\n",
    "        self.fc = nn.Linear(h_dim, o_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.dropout(self.emb(x))\n",
    "        print(embed)        \n",
    "        out, _ = self.lstm(embed)\n",
    "        hidden = self.fc(out[:, -1, :])\n",
    "        return hidden\n",
    "\n",
    "rnn_model = RNN(len(vocab),e_dim = 300,h_dim = 64, o_dim = 2)\n",
    "rnn_model.to(device)\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(),lr = lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "run_train(epochs,rnn_model,loader_train,loader_validation,optimizer,loss_func,model_type = \"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
