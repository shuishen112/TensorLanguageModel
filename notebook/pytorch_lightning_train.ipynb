{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pytorch lightning to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/wzm289/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b7d5a84a4794fbea3842fb62ade1ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 15696 words in dict\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "# preprocessing and tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import wandb\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def get_alphabet(corpuses):\n",
    "\t\"\"\"\n",
    "\tobtain the dict\n",
    "\t\t\t:param corpuses: \n",
    "\t\"\"\"\n",
    "\tword_counter = Counter()\n",
    "\n",
    "\tfor corpus in corpuses:\n",
    "\t\tfor item in corpus:\n",
    "\t\t\ttokens = tokenizer(item['sentence'])\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\tword_counter[token] += 1\n",
    "\tprint(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\t# logging.info(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\tword_dict = {word: e + 2 for e, word in enumerate(list(word_counter))}\n",
    "\tword_dict['UNK'] = 1\n",
    "\tword_dict['<PAD>'] = 0\n",
    "\n",
    "\treturn word_dict\n",
    "\n",
    "vocab = get_alphabet([dataset['train'],dataset['validation']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epch 100000\n",
      "epch 200000\n",
      "epch 300000\n",
      "epch 400000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# get embedding\n",
    "import numpy as np \n",
    "def get_embedding(alphabet, filename=\"\", embedding_size=100):\n",
    "\tembedding = np.random.rand(len(alphabet), embedding_size)\n",
    "\tif filename is None:\n",
    "\t\treturn embedding\n",
    "\twith open(filename, encoding='utf-8') as f:\n",
    "\t\ti = 0\n",
    "\t\tfor line in f:\n",
    "\t\t\ti += 1\n",
    "\t\t\tif i % 100000 == 0:\n",
    "\t\t\t\tprint('epch %d' % i)\n",
    "\t\t\titems = line.strip().split(' ')\n",
    "\t\t\tif len(items) == 2:\n",
    "\t\t\t\tvocab_size, embedding_size = items[0], items[1]\n",
    "\t\t\t\tprint((vocab_size, embedding_size))\n",
    "\t\t\telse:\n",
    "\t\t\t\tword = items[0]\n",
    "\t\t\t\tif word in alphabet:\n",
    "\t\t\t\t\tembedding[alphabet[word]] = items[1:]\n",
    "\n",
    "\tprint('done')\n",
    "\treturn embedding\n",
    "embedding = get_embedding(vocab, filename=\"../embedding/glove.6B.300d.txt\",embedding_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 111, 78, 470, 0, 0, 0, 0, 0, 0]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# convert to index\n",
    "\n",
    "def convert_to_word_ids(sentence,alphabet,max_len = 40):\n",
    "\t\"\"\"\n",
    "\tdocstring here\n",
    "\t\t:param sentence: \n",
    "\t\t:param alphabet: \n",
    "\t\t:param max_len=40: \n",
    "\t\"\"\"\n",
    "\tindices = []\n",
    "\ttokens = tokenizer(sentence)\n",
    "\t\n",
    "\tfor word in tokens:\n",
    "\t\tif word in alphabet:\n",
    "\t\t\tindices.append(alphabet[word])\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\tresult = indices + [alphabet['<PAD>']] * (max_len - len(indices))\n",
    "\n",
    "\treturn result[:max_len], min(len(tokens),max_len)\n",
    "\n",
    "test_enc, length = convert_to_word_ids(\"hello, how are you\", vocab, 10)\n",
    "print(test_enc)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data batch and iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "batch_size = 64\n",
    "class DataMaper(Dataset):\n",
    "    def __init__(self,dataset,vocab):\n",
    "        self.x = dataset['sentence']\n",
    "        self.y = dataset['label']\n",
    "        self.max_length = 40\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        enc_sentence,lengths = convert_to_word_ids(sentence, self.vocab, max_len = self.max_length)\n",
    "        t_sentence = torch.tensor(enc_sentence).to(device)\n",
    "        t_label = torch.tensor(label).to(device)\n",
    "        t_length = torch.tensor(lengths).to(device)\n",
    "        return t_sentence,t_label,t_length\n",
    "\n",
    "train = DataMaper(dataset['train'],vocab)\n",
    "validation = DataMaper(dataset['validation'],vocab)\n",
    "test = DataMaper(dataset['test'], vocab)\n",
    "\n",
    "loader_train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "loader_validation = DataLoader(validation, batch_size = batch_size)\n",
    "loader_test = DataLoader(test,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "def cal_accuracy(probs, target):\n",
    "    predictions = probs.argmax(dim=1)\n",
    "    corrects = (predictions == target)\n",
    "    accuracy = corrects.sum().float() / float(target.size(0))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the model to pytorch_lightning\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_dim, e_dim, h_dim, o_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_dim, e_dim, padding_idx=0)\n",
    "        self.emb.load_state_dict({\"weight\":torch.tensor(embedding)})\n",
    "        non_trainable = True\n",
    "        if non_trainable:\n",
    "            self.emb.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.conv1 = nn.Conv2d(1, h_dim, (3, e_dim))\n",
    "        self.conv2 = nn.Conv2d(1, h_dim, (4, e_dim))\n",
    "        self.conv3 = nn.Conv2d(1, h_dim, (5, e_dim))\n",
    "        self.fc = nn.Linear(h_dim * 3, o_dim)\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "        # self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.dropout(self.emb(x)).unsqueeze(1)\n",
    "        c1 = torch.relu(self.conv1(embed).squeeze(3))\n",
    "        p1 = torch.max_pool1d(c1, c1.size()[2]).squeeze(2)\n",
    "        c2 = torch.relu(self.conv2(embed).squeeze(3))\n",
    "        p2 = torch.max_pool1d(c2, c2.size()[2]).squeeze(2)\n",
    "        c3 = torch.relu(self.conv3(embed).squeeze(3))\n",
    "        p3 = torch.max_pool1d(c3, c3.size()[2]).squeeze(2)\n",
    "        pool = self.dropout(torch.cat((p1, p2, p3), 1))\n",
    "        hidden = self.fc(pool)\n",
    "        # return self.softmax(hidden), self.log_softmax(hidden)\n",
    "        return hidden \n",
    "\n",
    "class litCNN(pl.LightningModule):\n",
    "    def __init__(self, vocab_dim, e_dim, h_dim, o_dim):\n",
    "        super().__init__()\n",
    "        self.model = CNN(vocab_dim,e_dim,h_dim,o_dim)\n",
    "\n",
    "        # find the batch_size\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode = self.model(x)\n",
    "        return encode \n",
    "    \n",
    "    # optimizers go into configure_optimizer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr = 1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    # train and validation\n",
    "    def training_step(self,train_batch, batch_idx):\n",
    "        text, label,lengths = train_batch\n",
    "        predictions = self.model(text)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"train_loss\",loss)\n",
    "        self.log(\"acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,val_batch,batch_idx):\n",
    "        text, label,lengths = val_batch\n",
    "        predictions = self.model(text)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"val_loss\",loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        return acc\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        all_acc= torch.stack(validation_step_outputs)\n",
    "        print(torch.mean(all_acc))\n",
    "        self.log(\"val_epoch_acc\",torch.mean(all_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhansu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/zhansu/text_classification/runs/1k6m4tdq\" target=\"_blank\">cnn_word2vec</a></strong> to <a href=\"https://wandb.ai/zhansu/text_classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wzm289/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1823: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | CNN  | 4.9 M \n",
      "-------------------------------\n",
      "230 K     Trainable params\n",
      "4.7 M     Non-trainable params\n",
      "4.9 M     Total params\n",
      "19.762    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0042143d448b4672bfe53ec222621069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5312)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzm289/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/wzm289/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0fd4cba86c4d2aac0af24914708018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb_logger = WandbLogger(name = \"cnn_word2vec\",project=\"text_classification\")\n",
    "model = litCNN(len(vocab),e_dim = 300,h_dim = 64, o_dim = 2)\n",
    "# wandb_logger.watch(model, log=\"all\")\n",
    "trainer = pl.Trainer(logger=wandb_logger,max_epochs = 10)\n",
    "trainer.fit(model,loader_train,loader_validation)\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN pytorch_lightning training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhansu\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.19 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/zhansu/text_classification/runs/1l20kslc\" target=\"_blank\">rnn_word2vec</a></strong> to <a href=\"https://wandb.ai/zhansu/text_classification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(name = \"rnn_word2vec\",project=\"text_classification\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    # define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim , num_classes, lstm_layers,\n",
    "                 bidirectional, dropout, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
    "        self.embedding.load_state_dict({\"weight\":torch.tensor(embedding)})\n",
    "        non_trainable = True\n",
    "        if non_trainable:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_directions, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim)))\n",
    "        return h.to(device), c.to(device)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        batch_size = text.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu().numpy(), batch_first=True,enforce_sorted=False)\n",
    "        output, (h_n, c_n) = self.lstm(packed_embedded, (h_0, c_0))\n",
    "        # output_unpacked, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        # if it is bi directional LSTM, we should concat the two f\n",
    "        out = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
    "        # out = h_n[-1]\n",
    "        # print(h_n.shape)\n",
    "        # out = output_unpacked[:, -1, :]\n",
    "        preds = self.fc1(out)\n",
    "        return preds\n",
    "class litRNN(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes,lstm_layers, bidirectional, dropout, pad_index):\n",
    "        super().__init__()\n",
    "        self.model = LSTM(vocab_size = vocab_size,embedding_dim=embedding_dim,hidden_dim = hidden_dim,\n",
    "            num_classes = num_classes, lstm_layers = lstm_layers, bidirectional = bidirectional,dropout=dropout,pad_index = pad_index)\n",
    "\n",
    "        # find the batch_size\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        encode = self.model(x, lengths)\n",
    "        return encode \n",
    "    \n",
    "    # optimizers go into configure_optimizer\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr = 1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    # train and validation\n",
    "    def training_step(self,train_batch, batch_idx):\n",
    "        text, label, lengths = train_batch\n",
    "        predictions = self.model(text,lengths)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"train_loss\",loss)\n",
    "        self.log(\"acc\", acc)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self,val_batch,batch_idx):\n",
    "        text, label,lengths = val_batch\n",
    "        predictions = self.model(text,lengths)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        loss = criterion(predictions, label)\n",
    "        acc = cal_accuracy(predictions, label)\n",
    "        self.log(\"val_loss\",loss)\n",
    "        self.log(\"val_acc\", acc)\n",
    "        return acc\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        all_acc= torch.stack(validation_step_outputs)\n",
    "        print(torch.mean(all_acc))\n",
    "        self.log(\"val_epoch_acc\",torch.mean(all_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wzm289/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1823: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | LSTM | 5.3 M \n",
      "-------------------------------\n",
      "563 K     Trainable params\n",
      "4.7 M     Non-trainable params\n",
      "5.3 M     Total params\n",
      "21.092    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b3bac092183437084b41aa431e56bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzm289/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wzm289/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 96 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad1d2fc9b6f4d1abaac47e91992fefd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf274cbc725040638e61961ad4e7e2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8190)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821499d5b126454a80edf155e050fab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8277)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ff75eaaee2457bad8a7540c92ae5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8471)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6675c942aa0d4731aa5379a60c7ea5db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8442)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c1deddb76149e9888ee29c50bccc03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8397)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019e2f5663e8438c9a13864faa2e55fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8319)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90af5f4fc3dc4508b51c5a26852b80b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8304)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5eeaf14ed7747c5b2e22b4e0f66c9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8420)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32f283e3f72f40d191ee9b0c213a32db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8393)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc87f33e78df4f17b01fd12fb624d639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8444)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/pytorch_lightning_train.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/pytorch_lightning_train.ipynb#ch0000013vscode-remote?line=3'>4</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(logger\u001b[39m=\u001b[39mwandb_logger,max_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/pytorch_lightning_train.ipynb#ch0000013vscode-remote?line=4'>5</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model,loader_train,loader_validation)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bfutharkhpa01fl.unicph.domain/projects/futhark1/data/wzm289/code/MulTeacher-KD/notebook/pytorch_lightning_train.ipynb#ch0000013vscode-remote?line=5'>6</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = litRNN(vocab_size = len(vocab),embedding_dim=300,hidden_dim = 100,\n",
    "num_classes = 2, lstm_layers = 2, bidirectional = True,dropout=0.5,pad_index = 0)\n",
    "# wandb_logger.watch(model, log=\"all\")\n",
    "trainer = pl.Trainer(logger=wandb_logger,max_epochs = 10)\n",
    "trainer.fit(model,loader_train,loader_validation)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
