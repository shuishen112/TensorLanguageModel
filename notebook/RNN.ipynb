{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/wzm289/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca50906e2d1413f991046e6d6adf7ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "import torch\n",
    "\n",
    "from torchtext import data\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 15696 words in dict\n"
     ]
    }
   ],
   "source": [
    "# preprocessing and tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def get_alphabet(corpuses):\n",
    "\t\"\"\"\n",
    "\tobtain the dict\n",
    "\t\t\t:param corpuses: \n",
    "\t\"\"\"\n",
    "\tword_counter = Counter()\n",
    "\n",
    "\tfor corpus in corpuses:\n",
    "\t\tfor item in corpus:\n",
    "\t\t\ttokens = tokenizer(item['sentence'])\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\tword_counter[token] += 1\n",
    "\tprint(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\t# logging.info(\"there are {} words in dict\".format(len(word_counter)))\n",
    "\tword_dict = {word: e + 2 for e, word in enumerate(list(word_counter))}\n",
    "\tword_dict['UNK'] = 1\n",
    "\tword_dict['<PAD>'] = 0\n",
    "\n",
    "\treturn word_dict\n",
    "\n",
    "vocab = get_alphabet([dataset['train'],dataset['validation']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epch 100000\n",
      "epch 200000\n",
      "epch 300000\n",
      "epch 400000\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# get embedding\n",
    "import numpy as np \n",
    "def get_embedding(alphabet, filename=\"\", embedding_size=100):\n",
    "\tembedding = np.random.rand(len(alphabet), embedding_size)\n",
    "\tif filename is None:\n",
    "\t\treturn embedding\n",
    "\twith open(filename, encoding='utf-8') as f:\n",
    "\t\ti = 0\n",
    "\t\tfor line in f:\n",
    "\t\t\ti += 1\n",
    "\t\t\tif i % 100000 == 0:\n",
    "\t\t\t\tprint('epch %d' % i)\n",
    "\t\t\titems = line.strip().split(' ')\n",
    "\t\t\tif len(items) == 2:\n",
    "\t\t\t\tvocab_size, embedding_size = items[0], items[1]\n",
    "\t\t\t\tprint((vocab_size, embedding_size))\n",
    "\t\t\telse:\n",
    "\t\t\t\tword = items[0]\n",
    "\t\t\t\tif word in alphabet:\n",
    "\t\t\t\t\tembedding[alphabet[word]] = items[1:]\n",
    "\n",
    "\tprint('done')\n",
    "\treturn embedding\n",
    "embedding = get_embedding(vocab, filename=\"../embedding/glove.6B.300d.txt\",embedding_size = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15698, 300)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 111, 78, 470, 0, 0, 0, 0, 0, 0]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# convert to index\n",
    "\n",
    "def convert_to_word_ids(sentence,alphabet,max_len = 40):\n",
    "\t\"\"\"\n",
    "\tdocstring here\n",
    "\t\t:param sentence: \n",
    "\t\t:param alphabet: \n",
    "\t\t:param max_len=40: \n",
    "\t\"\"\"\n",
    "\tindices = []\n",
    "\ttokens = tokenizer(sentence)\n",
    "\t\n",
    "\tfor word in tokens:\n",
    "\t\tif word in alphabet:\n",
    "\t\t\tindices.append(alphabet[word])\n",
    "\t\telse:\n",
    "\t\t\tcontinue\n",
    "\tresult = indices + [alphabet['<PAD>']] * (max_len - len(indices))\n",
    "\n",
    "\treturn result[:max_len], min(len(tokens),max_len)\n",
    "\n",
    "test_enc, length = convert_to_word_ids(\"hello, how are you\", vocab, 10)\n",
    "print(test_enc)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data batch and iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 64\n",
    "class DataMaper(Dataset):\n",
    "    def __init__(self,dataset,vocab):\n",
    "        self.x = dataset['sentence']\n",
    "        self.y = dataset['label']\n",
    "        self.max_length = 20\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.x[idx]\n",
    "        label = self.y[idx]\n",
    "\n",
    "        enc_sentence,lengths = convert_to_word_ids(sentence, self.vocab, max_len = self.max_length)\n",
    "        t_sentence = torch.tensor(enc_sentence).to(device)\n",
    "        t_label = torch.tensor(label).to(device)\n",
    "        t_length = torch.tensor(lengths).to(device)\n",
    "        return t_sentence,t_label,t_length\n",
    "\n",
    "train = DataMaper(dataset['train'],vocab)\n",
    "validation = DataMaper(dataset['validation'],vocab)\n",
    "test = DataMaper(dataset['test'], vocab)\n",
    "\n",
    "loader_train = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "loader_validation = DataLoader(validation, batch_size = batch_size)\n",
    "loader_test = DataLoader(test,batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in loader_train:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(probs, target):\n",
    "    predictions = probs.argmax(dim=1)\n",
    "    corrects = (predictions == target)\n",
    "    accuracy = corrects.sum().float() / float(target.size(0))\n",
    "    return accuracy\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, label,lengths = batch\n",
    "        predictions = model(text,lengths)\n",
    "       \n",
    "        loss = criterion(predictions, label.squeeze())\n",
    "        acc = cal_accuracy(predictions, label)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, label,lengths = batch\n",
    "            predictions = model(text,lengths).squeeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            acc = cal_accuracy(predictions, label)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(epochs, model, train_iterator, valid_iterator, optimizer, criterion, model_type):\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # train the model\n",
    "        print(\"train epoch:{}\".format(epoch))\n",
    "        model.train()\n",
    "        train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "\n",
    "        # evaluate the model\n",
    "        valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "        # save the best model\n",
    "        # if valid_loss < best_valid_loss:\n",
    "        #     best_valid_loss = valid_loss\n",
    "        #     torch.save(model.state_dict(), 'saved_weights'+'_'+model_type+'.pt')\n",
    "\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    # define all the layers used in model\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim , num_classes, lstm_layers,\n",
    "                 bidirectional, dropout, pad_index):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_index)\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=lstm_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.fc1 = nn.Linear(hidden_dim * num_directions, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm_layers = lstm_layers\n",
    "        self.num_directions = num_directions\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h, c = (Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim)),\n",
    "                Variable(torch.zeros(self.lstm_layers * self.num_directions, batch_size, self.hidden_dim)))\n",
    "        return h.to(device), c.to(device)\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        batch_size = text.shape[0]\n",
    "        h_0, c_0 = self.init_hidden(batch_size)\n",
    "\n",
    "        embedded = self.embedding(text)\n",
    "        packed_embedded = pack_padded_sequence(embedded, text_lengths.cpu().numpy(), batch_first=True,enforce_sorted=False)\n",
    "        output, (h_n, c_n) = self.lstm(packed_embedded, (h_0, c_0))\n",
    "        # output_unpacked, output_lengths = pad_packed_sequence(output, batch_first=True)\n",
    "        # if it is bi directional LSTM, we should concat the two f\n",
    "        out = torch.cat((h_n[-2, :, :], h_n[-1, :, :]), dim=1)\n",
    "        # out = h_n[-1]\n",
    "        # print(h_n.shape)\n",
    "        # out = output_unpacked[:, -1, :]\n",
    "        preds = self.fc1(out)\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 10\n",
    "# lr = 1e-2\n",
    "\n",
    "# rnn_model = LSTM(vocab_size = len(vocab),embedding_dim=300,hidden_dim = 100,\n",
    "# num_classes = 2, lstm_layers = 2, bidirectional = True,dropout=0.5,pad_index = 0)\n",
    "# rnn_model.to(device)\n",
    "# optimizer = torch.optim.Adam(rnn_model.parameters(),lr = lr)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "# run_train(epochs,rnn_model,loader_train,loader_validation,optimizer,loss_func,model_type = \"cls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiplicative RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    # you can also accept arguments in your model constructor\n",
    "\n",
    "    #  we don't use the output in this implemention\n",
    "    def __init__(self, embed_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        input_size = embed_size + hidden_size\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        # self.Wih = nn.Parameter(torch.FloatTensor(embed_size,hidden_size))\n",
    "        # self.Whh = nn.Parameter(torch.FloatTensor(hidden_size,hidden_size))\n",
    "        self.Wih = nn.Linear(embed_size, hidden_size)\n",
    "        self.Whh = nn.Linear(hidden_size, hidden_size)\n",
    "        # w_im = torch.Tensor(embed_size,  hidden_size)\n",
    "        # w_hm = torch.Tensor(hidden_size, hidden_size)\n",
    "        # b_im = torch.Tensor(hidden_size)\n",
    "        # b_hm = torch.Tensor(hidden_size)\n",
    "        # self.w_im = nn.Parameter(w_im)\n",
    "        # self.b_im = nn.Parameter(b_im)\n",
    "        # self.w_hm = nn.Parameter(w_hm)\n",
    "        # self.b_hm = nn.Parameter(b_hm)\n",
    "        self.w_im = nn.Linear(embed_size, hidden_size)\n",
    "        self.w_hm = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # self.i2h = nn.Linear(input_size, hidden_size)\n",
    "        self.h2o = nn.Linear(input_size, output_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "        \n",
    "\n",
    "    def forward(self, data, last_hidden):\n",
    "        input = torch.cat((data, last_hidden), 1)\n",
    "    \n",
    "        mx = self.w_im(data) * self.w_hm(last_hidden)\n",
    "\n",
    "        wi = self.Wih(data)\n",
    "        wh = self.Whh(mx)\n",
    "        # wi = torch.mm(data,self.Wih)\n",
    "        # wh = torch.mm(mx,self.Whh)\n",
    "\n",
    "        hidden = torch.relu(wi + wh)\n",
    "\n",
    "        output = self.h2o(input)\n",
    "        return output, hidden\n",
    "    def initHidden(self,batch_size):\n",
    "        # return torch.zeros(batch_size,self.hidden_size).to(self.device)\n",
    "        return nn.init.kaiming_uniform_(torch.empty(batch_size, self.hidden_size)).to(self.device)\n",
    "class RNN_layer(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,hidden_dim,output_size):\n",
    "        super(RNN_layer,self).__init__()\n",
    "        self.rnn = RNN(embed_size,hidden_dim,output_size)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size,padding_idx=0)\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self,x):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        x = self.dropout(self.embedding(x))\n",
    "\n",
    "        hidden = self.rnn.initHidden(batch_size)\n",
    "        hiddens = []\n",
    "        # recurrent rnn\n",
    "        for i in range(seq_len):\n",
    "            output, hidden = self.rnn(x[:,i,:], hidden)\n",
    "            hiddens.append(hidden.unsqueeze(1))\n",
    "        final_hidden = hidden\n",
    "        hidden_tensor = torch.cat(hiddens,1)\n",
    "        return hidden_tensor,final_hidden,output\n",
    "        \n",
    "\n",
    "class RNN_Model_for_classfication(nn.Module):\n",
    "    def __init__(self,vocab_size,embed_size,hidden_dim,output_size):\n",
    "        super(RNN_Model_for_classfication,self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.vocab_size = vocab_size \n",
    "        # define the layer\n",
    "        # self.rnn = nn.RNN(embed_size,hidden_dim,num_layers = 1,batch_first= True)\n",
    "        self.rnn = RNN_layer(self.vocab_size,embed_size,hidden_dim,output_size)\n",
    "        self.fc = nn.Linear(hidden_dim,output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "    def forward(self,x,lens):\n",
    "    \n",
    "        hidden_tensor, final_hidden , output = self.rnn(x)\n",
    "\n",
    "        out = output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train epoch:0\n",
      "\tTrain Loss: 0.693 | Train Acc: 56.05%\n",
      "\t Val. Loss: 0.704 |  Val. Acc: 51.70%\n",
      "train epoch:1\n",
      "\tTrain Loss: 0.680 | Train Acc: 57.63%\n",
      "\t Val. Loss: 0.726 |  Val. Acc: 52.32%\n",
      "train epoch:2\n",
      "\tTrain Loss: 0.669 | Train Acc: 58.64%\n",
      "\t Val. Loss: 0.773 |  Val. Acc: 52.23%\n",
      "train epoch:3\n",
      "\tTrain Loss: 0.659 | Train Acc: 59.40%\n",
      "\t Val. Loss: 0.763 |  Val. Acc: 53.19%\n",
      "train epoch:4\n",
      "\tTrain Loss: 823598.447 | Train Acc: 57.19%\n",
      "\t Val. Loss: 2.446 |  Val. Acc: 50.36%\n",
      "train epoch:5\n",
      "\tTrain Loss: 19.433 | Train Acc: 57.28%\n",
      "\t Val. Loss: 2.199 |  Val. Acc: 51.58%\n",
      "train epoch:6\n",
      "\tTrain Loss: 9.246 | Train Acc: 57.56%\n",
      "\t Val. Loss: 3.100 |  Val. Acc: 51.09%\n",
      "train epoch:7\n",
      "\tTrain Loss: 2.250 | Train Acc: 57.87%\n",
      "\t Val. Loss: 2.201 |  Val. Acc: 50.87%\n",
      "train epoch:8\n",
      "\tTrain Loss: 1.516 | Train Acc: 58.00%\n",
      "\t Val. Loss: 1.563 |  Val. Acc: 50.31%\n",
      "train epoch:9\n",
      "\tTrain Loss: 1.222 | Train Acc: 58.12%\n",
      "\t Val. Loss: 1.364 |  Val. Acc: 50.83%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "rnn_model = RNN_Model_for_classfication(vocab_size = len(vocab),embed_size = 300,hidden_dim = 256,\n",
    "output_size = 2)\n",
    "\n",
    "rnn_model.to(device)\n",
    "optimizer = torch.optim.Adam(rnn_model.parameters(),lr = lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "run_train(epochs,rnn_model,loader_train,loader_validation,optimizer,loss_func,model_type = \"cls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
