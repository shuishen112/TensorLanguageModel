{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from torch import nn\n","\n","import numpy as np\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["text = ['hey how are you','good i am fine','have a nice day']\n","\n","chars = set(''.join(text))\n","int2char = dict(enumerate(chars))\n","char2int = {char: ind for ind,char in int2char.items()}\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The longest string has 15 characters\n"]}],"source":["maxlen = len(max(text, key=len))\n","print(\"The longest string has {} characters\".format(maxlen))\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# padding the text\n","for i in range(len(text)):\n","    while len(text[i]) < maxlen:\n","        text[i] += ' '\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input Sequence:hey how are yo\n"," Target Sequence:ey how are you\n","Input Sequence:good i am fine\n"," Target Sequence:ood i am fine \n","Input Sequence:have a nice da\n"," Target Sequence:ave a nice day\n"]}],"source":["input_seq = []\n","target_seq = []\n","\n","for i in range(len(text)):\n","    # remove the first token\n","\n","    input_seq.append(text[i][:-1])\n","    target_seq.append(text[i][1:])\n","    print(\"Input Sequence:{}\\n Target Sequence:{}\".format(\n","        input_seq[i],\n","        target_seq[i]\n","    ))\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["for i in range(len(text)):\n","    input_seq[i] = [char2int[character] for character in input_seq[i]]\n","    target_seq[i] = [char2int[character] for character in target_seq[i]]\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["dict_size = len(char2int)\n","seq_len = maxlen - 1\n","batch_size = len(text)\n","\n","def one_hot_encode(sequence,dict_size,seq_len,batch_size):\n","    features = np.zeros((batch_size,seq_len,dict_size),dtype = np.float32)\n","\n","    for i in range(batch_size):\n","        for u in range(seq_len):\n","            features[i,u,sequence[i][u]] = 1\n","    return features\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Input shape: torch.Size([3, 14]) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"]}],"source":["input_seq = torch.tensor(input_seq)\n","target_seq = torch.tensor(target_seq)\n","\n","print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n"]}],"source":["# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n","is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["\n","class TN(nn.Module):\n","\n","    # tensor network unit\n","    def __init__(self, rank, output_size):\n","        super(TN, self).__init__()\n","\n","        self.rank = rank\n","        self.output_size = output_size\n","        input_size = rank + rank\n","\n","        self.i2h = nn.Linear(self.rank, self.rank)\n","        self.h2o = nn.Linear(self.rank, output_size)\n","    \n","\n","    def forward(self, data, m):\n","        # input = torch.cat((data, m.squeeze(1)), 1)\n","\n","        # hidden = self.i2h(input)\n","        # output = self.h2o(hidden)\n","\n","        # unit = self.i2h(data)\n","\n","        unit = data.contiguous().view(-1,self.rank,self.rank)\n","        # get hidden\n","        activition = torch.nn.Tanh()\n","        m = torch.einsum(\"bij,bjk->bik\",[m,unit])\n","        # # m = unit\n","        hidden = activition(self.i2h(m))\n","        output = self.h2o(hidden)\n","        return hidden, output\n","\n","    def init_m1(self):\n","        return nn.Linear(1,self.rank).to(device)\n","    def init_m2(self):\n","        return nn.Linear(self.rank, self.output_size)\n","    def init_hidden(self,batch_size):\n","        return torch.zeros(batch_size,self.rank).to(device)\n","\n","class TN_layer(nn.Module):\n","    def __init__(self,rank,output_size):\n","        super(TN_layer,self).__init__()\n","        self.tn = TN(rank,output_size)\n","        self.rank = rank\n","        self.embedding = nn.Embedding(output_size,self.rank * self.rank)\n","\n","        \n","    def forward(self,x):\n","        batch_size = x.size(0)\n","        seq_len = x.size(1)\n","\n","        encoding = self.embedding(x)\n","        \n","        # m = self.tn.init_hidden(batch_size)\n","        m = self.tn.init_m1()\n","        m = m.weight.view(-1,self.rank).unsqueeze(0).repeat([batch_size,1,1])\n","        hiddens = []\n","        # recurrent tn\n","        for i in range(seq_len):\n","            m, output = self.tn(encoding[:,i,:], m)\n","            hiddens.append(m)\n","        final_hidden = m\n","        hidden_tensor = torch.cat(hiddens,1)\n","        return hidden_tensor,final_hidden\n","        \n","\n","class Model(nn.Module):\n","    def __init__(self,rank,output_size,):\n","        super(Model,self).__init__()\n","\n","        self.rank = rank\n","\n","        self.tn = TN_layer(self.rank,output_size)\n","        self.fc = nn.Linear(self.rank,output_size)\n","\n","    def forward(self,x):\n","        out, hidden = self.tn(x)\n","        out = out.contiguous().view(-1,self.rank)\n","        out = self.fc(out)\n","        return out, hidden\n","    def init_hidden(self,batch_size):\n","        hidden = torch.zeros(self.n_layers,batch_size,self.hidden_dim).to(device)\n","        return hidden\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["model = Model(rank = 12,output_size = dict_size)\n","\n","model = model.to(device)\n","\n","n_epochs = 1000\n","lr = 0.01\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 10/1000............. Loss: 2.5180\n","Epoch: 20/1000............. Loss: 2.3258\n","Epoch: 30/1000............. Loss: 2.0516\n","Epoch: 40/1000............. Loss: 1.8373\n","Epoch: 50/1000............. Loss: 2.1017\n","Epoch: 60/1000............. Loss: 1.5595\n","Epoch: 70/1000............. Loss: 1.3359\n","Epoch: 80/1000............. Loss: 1.1504\n","Epoch: 90/1000............. Loss: 1.1604\n","Epoch: 100/1000............. Loss: 0.9977\n","Epoch: 110/1000............. Loss: 0.9025\n","Epoch: 120/1000............. Loss: 0.7761\n","Epoch: 130/1000............. Loss: 0.7602\n","Epoch: 140/1000............. Loss: 0.6830\n","Epoch: 150/1000............. Loss: 0.6661\n","Epoch: 160/1000............. Loss: 0.6974\n","Epoch: 170/1000............. Loss: 0.4937\n","Epoch: 180/1000............. Loss: 0.4742\n","Epoch: 190/1000............. Loss: 0.4222\n","Epoch: 200/1000............. Loss: 0.6135\n","Epoch: 210/1000............. Loss: 0.3146\n","Epoch: 220/1000............. Loss: 0.3468\n","Epoch: 230/1000............. Loss: 0.3116\n","Epoch: 240/1000............. Loss: 0.2411\n","Epoch: 250/1000............. Loss: 0.3626\n","Epoch: 260/1000............. Loss: 0.2547\n","Epoch: 270/1000............. Loss: 0.2555\n","Epoch: 280/1000............. Loss: 0.2692\n","Epoch: 290/1000............. Loss: 0.2595\n","Epoch: 300/1000............. Loss: 0.1264\n","Epoch: 310/1000............. Loss: 0.2072\n","Epoch: 320/1000............. Loss: 0.1512\n","Epoch: 330/1000............. Loss: 0.1344\n","Epoch: 340/1000............. Loss: 0.1360\n","Epoch: 350/1000............. Loss: 0.1459\n","Epoch: 360/1000............. Loss: 0.1462\n","Epoch: 370/1000............. Loss: 0.1193\n","Epoch: 380/1000............. Loss: 0.1140\n","Epoch: 390/1000............. Loss: 0.1065\n","Epoch: 400/1000............. Loss: 0.1817\n","Epoch: 410/1000............. Loss: 0.1317\n","Epoch: 420/1000............. Loss: 0.0964\n","Epoch: 430/1000............. Loss: 0.0891\n","Epoch: 440/1000............. Loss: 0.1477\n","Epoch: 450/1000............. Loss: 0.1159\n","Epoch: 460/1000............. Loss: 0.0833\n","Epoch: 470/1000............. Loss: 0.1164\n","Epoch: 480/1000............. Loss: 0.1544\n","Epoch: 490/1000............. Loss: 0.1580\n","Epoch: 500/1000............. Loss: 0.1127\n","Epoch: 510/1000............. Loss: 0.1444\n","Epoch: 520/1000............. Loss: 0.1122\n","Epoch: 530/1000............. Loss: 0.0838\n","Epoch: 540/1000............. Loss: 0.0757\n","Epoch: 550/1000............. Loss: 0.1206\n","Epoch: 560/1000............. Loss: 0.0868\n","Epoch: 570/1000............. Loss: 0.0939\n","Epoch: 580/1000............. Loss: 0.0663\n","Epoch: 590/1000............. Loss: 0.1244\n","Epoch: 600/1000............. Loss: 0.0828\n","Epoch: 610/1000............. Loss: 0.0978\n","Epoch: 620/1000............. Loss: 0.0833\n","Epoch: 630/1000............. Loss: 0.0793\n","Epoch: 640/1000............. Loss: 0.0819\n","Epoch: 650/1000............. Loss: 0.0712\n","Epoch: 660/1000............. Loss: 0.1306\n","Epoch: 670/1000............. Loss: 0.0842\n","Epoch: 680/1000............. Loss: 0.0683\n","Epoch: 690/1000............. Loss: 0.0804\n","Epoch: 700/1000............. Loss: 0.0873\n","Epoch: 710/1000............. Loss: 0.0670\n","Epoch: 720/1000............. Loss: 0.1373\n","Epoch: 730/1000............. Loss: 0.0737\n","Epoch: 740/1000............. Loss: 0.0641\n","Epoch: 750/1000............. Loss: 0.0528\n","Epoch: 760/1000............. Loss: 0.0809\n","Epoch: 770/1000............. Loss: 0.0477\n","Epoch: 780/1000............. Loss: 0.0752\n","Epoch: 790/1000............. Loss: 0.0632\n","Epoch: 800/1000............. Loss: 0.0557\n","Epoch: 810/1000............. Loss: 0.0601\n","Epoch: 820/1000............. Loss: 0.0558\n","Epoch: 830/1000............. Loss: 0.0612\n","Epoch: 840/1000............. Loss: 0.0817\n","Epoch: 850/1000............. Loss: 0.0597\n","Epoch: 860/1000............. Loss: 0.0526\n","Epoch: 870/1000............. Loss: 0.0528\n","Epoch: 880/1000............. Loss: 0.0486\n","Epoch: 890/1000............. Loss: 0.0625\n","Epoch: 900/1000............. Loss: 0.0533\n","Epoch: 910/1000............. Loss: 0.0957\n","Epoch: 920/1000............. Loss: 0.0542\n","Epoch: 930/1000............. Loss: 0.0553\n","Epoch: 940/1000............. Loss: 0.3105\n","Epoch: 950/1000............. Loss: 0.0963\n","Epoch: 960/1000............. Loss: 0.4162\n","Epoch: 970/1000............. Loss: 0.8976\n","Epoch: 980/1000............. Loss: 0.0779\n","Epoch: 990/1000............. Loss: 0.1359\n","Epoch: 1000/1000............. Loss: 0.0862\n"]}],"source":["input_seq = input_seq.to(device)\n","for epoch in range(1, n_epochs + 1):\n","    optimizer.zero_grad() # Clears existing gradients from previous epoch\n","    #input_seq = input_seq.to(device)\n","    output, hidden = model(input_seq)\n","    output = output.to(device)\n","    target_seq = target_seq.to(device)\n","    loss = criterion(output.view(-1,dict_size), target_seq.view(-1).long())\n","    loss.backward() # Does backpropagation and calculates gradients\n","    optimizer.step() # Updates the weights accordingly\n","    \n","    if epoch%10 == 0:\n","        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n","        print(\"Loss: {:.4f}\".format(loss.item()))\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def predict(model, character):\n","    # One-hot encoding our input to fit into the model\n","    character = np.array([[char2int[c] for c in character]])\n","    character = torch.from_numpy(character)\n","    character = character.to(device) \n","    out, hidden = model(character)\n","    out = out.squeeze(0)\n","    prob = nn.functional.softmax(out[-1], dim=0).data\n","    # Taking the class with the highest probability score from the output\n","    char_ind = torch.max(prob, dim=0)[1].item()\n","    print(char_ind)\n","\n","    return int2char[char_ind], hidden\n","def sample(model, out_len, start='hey'):\n","    model.eval() # eval mode\n","    start = start.lower()\n","    # First off, run through the starting characters\n","    chars = [ch for ch in start]\n","    size = out_len - len(chars)\n","    # Now pass in the previous characters and get a new one\n","    for ii in range(size):\n","        char, h = predict(model, chars)\n","        chars.append(char)\n","\n","    return ''.join(chars)\n"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["15\n","4\n","9\n","5\n","15\n","12\n","16\n","13\n","15\n","10\n","9\n","0\n","hey how are you\n"]}],"source":["print(sample(model, 15, 'hey'))\n"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n"]}],"metadata":{"interpreter":{"hash":"d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"},"kernelspec":{"display_name":"Python 3.9.5 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
