{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build the language model dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text8 dataset\n",
    "\n",
    "text8 dataset is a long string. we should use text8 to build a langauge model task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "with open(\"../data/text8\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.read()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the train val and test dataset\n",
    "num_test_chars = 5000000\n",
    "train_data = data[: -2 * num_test_chars]\n",
    "valid_data = data[-2 * num_test_chars : -num_test_chars]\n",
    "test_data = data[-num_test_chars:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume the data is a long string which has to be divied into batches containing examples of\n",
    "from math import ceil\n",
    "from tqdm import tqdm\n",
    "def _split_example(data, *, number_of_splits=None, length_of_each_split=None):\n",
    "    \"\"\"\n",
    "    Divides the string into n parts. If it's not possible to divide the string into n EQUAL parts,\n",
    "    then nth part will be smaller than the first n-1 parts(which will all be of equal lengths)\n",
    "    :param data: The string to be divided into n parts\n",
    "    :param number_of_splits: The number of parts to divide the string into\n",
    "    \"\"\"\n",
    "    if not number_of_splits and not length_of_each_split:\n",
    "        raise ValueError(\n",
    "            \"At least one of the two keyword arguments must be provided\"\n",
    "        )\n",
    "    split_length = length_of_each_split or ceil(len(data) / number_of_splits)\n",
    "    num_splits = number_of_splits or ceil(len(data) / split_length)\n",
    "    return [\n",
    "        data[i * split_length : min(len(data), (i + 1) * split_length)]\n",
    "        for i in range(num_splits)\n",
    "    ]\n",
    "x = _split_example(train_data, number_of_splits=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "703125\n"
     ]
    }
   ],
   "source": [
    "print(len(x))\n",
    "print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab\n",
    "import string\n",
    "string.ascii_lowercase + \" \"\n",
    "\n",
    "END_OF_SENTENCE_TOKEN = \"<EOS>\"\n",
    "OUT_OF_VOCAB_TOKEN = \"<OOV>\"\n",
    "PADDING_TOKEN = \"<PAD>\"\n",
    "vocab = [END_OF_SENTENCE_TOKEN,OUT_OF_VOCAB_TOKEN,PADDING_TOKEN]\n",
    "vocab = vocab + list(set(string.ascii_lowercase + \" \"))\n",
    "_vocab = {token: id for id, token in enumerate(vocab)}\n",
    "_inverse_vocab = {id:token for id, token in enumerate(vocab)}\n",
    "\n",
    "def token2id(token):\n",
    "    return _vocab.get(token,_vocab[OUT_OF_VOCAB_TOKEN])\n",
    "def id2token(_id):\n",
    "    return _inverse_vocab.get(_id, OUT_OF_VOCAB_TOKEN)\n",
    "def map_tokens_to_ids(tokens):\n",
    "    return [token2id(token) for token in tokens]\n",
    "def tokenize(data: list):\n",
    "    \"\"\"Maps characters to integers\"\"\"\n",
    "    return [map_tokens_to_ids(tokens) for tokens in tqdm(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 128/128 [00:15<00:00,  8.10it/s]\n"
     ]
    }
   ],
   "source": [
    "x_tokenize =  tokenize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the target sequence \n",
    "def create_target_sequences(data: list):\n",
    "    return [x[1:] + [token2id(END_OF_SENTENCE_TOKEN)] for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = create_target_sequences(x_tokenize)\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:06<00:00, 20.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# now x has a single batch each batch in x is very long, so we need to split the exmple \n",
    "# into n parts. \n",
    "sequence_length = 100\n",
    "\n",
    "x_tokenize_split = [\n",
    "    _split_example(example, length_of_each_split=sequence_length)\n",
    "    for example in tqdm(x_tokenize)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [00:00<00:00, 2990.51it/s]\n"
     ]
    }
   ],
   "source": [
    "y = [\n",
    "    _split_example(example, length_of_each_split=sequence_length)\n",
    "        for example in tqdm(y)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])\n",
    "len(y[67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7032\n"
     ]
    }
   ],
   "source": [
    "# build the train dataset\n",
    "batch_size = 128\n",
    "max_number_of_splits = max((len(i) for i in x_tokenize_split))\n",
    "\n",
    "print(max_number_of_splits)\n",
    "# X_train = []\n",
    "# y_train = []\n",
    "# lengths = []\n",
    "# for i in range(max_number_of_splits):\n",
    "#     for j in range(batch_size):\n",
    "#         if i < len(x_tokenize_split[j]):\n",
    "#             X_train.append(x_tokenize_split[j][i])\n",
    "           \n",
    "#             y_train.append(y[j][i])\n",
    "#             lengths.append(len(x_tokenize_split[j][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x_tokenize_split:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d8ed0073c41b023cdd22b6268c6b3f0c9b6d97a6234cf243afa672a36c79f6c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
