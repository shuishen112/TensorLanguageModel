{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from turtle import TNavigator\n","import torch\n","from torch import nn\n","\n","import numpy as np\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text = ['hey how are you','good i am fine','have a nice day']\n","\n","chars = set(''.join(text))\n","int2char = dict(enumerate(chars))\n","char2int = {char: ind for ind,char in int2char.items()}\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["maxlen = len(max(text, key=len))\n","print(\"The longest string has {} characters\".format(maxlen))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# padding the text\n","for i in range(len(text)):\n","    while len(text[i]) < maxlen:\n","        text[i] += ' '\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_seq = []\n","target_seq = []\n","\n","for i in range(len(text)):\n","    # remove the first token\n","\n","    input_seq.append(text[i][:-1])\n","    target_seq.append(text[i][1:])\n","    print(\"Input Sequence:{}\\n Target Sequence:{}\".format(\n","        input_seq[i],\n","        target_seq[i]\n","    ))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(len(text)):\n","    input_seq[i] = [char2int[character] for character in input_seq[i]]\n","    target_seq[i] = [char2int[character] for character in target_seq[i]]\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dict_size = len(char2int)\n","seq_len = maxlen - 1\n","batch_size = len(text)\n","\n","def one_hot_encode(sequence,dict_size,seq_len,batch_size):\n","    features = np.zeros((batch_size,seq_len,dict_size),dtype = np.float32)\n","\n","    for i in range(batch_size):\n","        for u in range(seq_len):\n","            features[i,u,sequence[i][u]] = 1\n","    return features\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_seq = torch.tensor(input_seq)\n","target_seq = torch.tensor(target_seq)\n","\n","print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n","is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","class TN(nn.Module):\n","\n","    # tensor network unit\n","    def __init__(self, rank, output_size):\n","        super(TN, self).__init__()\n","\n","        self.rank = rank\n","        self.output_size = output_size\n","        input_size = rank + rank\n","\n","        self.i2h = nn.Linear(self.rank, self.rank)\n","        self.h2o = nn.Linear(self.rank, output_size)\n","    \n","\n","    def forward(self, data, m):\n","        # input = torch.cat((data, m.squeeze(1)), 1)\n","\n","        # hidden = self.i2h(input)\n","        # output = self.h2o(hidden)\n","\n","        # unit = self.i2h(data)\n","\n","        unit = data.contiguous().view(-1,self.rank,self.rank)\n","        # get hidden\n","        activition = torch.nn.Tanh()\n","        m = torch.einsum(\"bij,bjk->bik\",[m,unit])\n","        # # m = unit\n","        hidden = activition(self.i2h(m))\n","        output = self.h2o(hidden)\n","        return hidden, output\n","\n","    def init_m1(self):\n","        return nn.Linear(1,self.rank).to(device)\n","    def init_m2(self):\n","        return nn.Linear(self.rank, self.output_size)\n","    def init_hidden(self,batch_size):\n","        return torch.zeros(batch_size,self.rank).to(device)\n","\n","class TN_layer(nn.Module):\n","    def __init__(self,rank,output_size):\n","        super(TN_layer,self).__init__()\n","        self.tn = TN(rank,output_size)\n","        self.rank = rank\n","        self.embedding = nn.Embedding(output_size,self.rank * self.rank)\n","\n","        \n","    def forward(self,x):\n","        batch_size = x.size(0)\n","        seq_len = x.size(1)\n","\n","        encoding = self.embedding(x)\n","        \n","        # m = self.tn.init_hidden(batch_size)\n","        m = self.tn.init_m1()\n","        m = m.weight.view(-1,self.rank).unsqueeze(0).repeat([batch_size,1,1])\n","        hiddens = []\n","        # recurrent tn\n","        for i in range(seq_len):\n","            m, output = self.tn(encoding[:,i,:], m)\n","            hiddens.append(m)\n","        final_hidden = m\n","        hidden_tensor = torch.cat(hiddens,1)\n","        return hidden_tensor,final_hidden\n","        \n","\n","class Model(nn.Module):\n","    def __init__(self,rank,output_size,):\n","        super(Model,self).__init__()\n","\n","        self.rank = rank\n","\n","        self.tn = TN_layer(self.rank,output_size)\n","        self.fc = nn.Linear(self.rank,output_size)\n","\n","    def forward(self,x):\n","        out, hidden = self.tn(x)\n","        out = out.contiguous().view(-1,self.rank)\n","        out = self.fc(out)\n","        return out, hidden\n","    def init_hidden(self,batch_size):\n","        hidden = torch.zeros(self.n_layers,batch_size,self.hidden_dim).to(device)\n","        return hidden\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = Model(rank = 12,output_size = dict_size)\n","\n","model = model.to(device)\n","\n","n_epochs = 1000\n","lr = 0.01\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),lr = lr)\n","\n","\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_seq = input_seq.to(device)\n","for epoch in range(1, n_epochs + 1):\n","    optimizer.zero_grad() # Clears existing gradients from previous epoch\n","    #input_seq = input_seq.to(device)\n","    output, hidden = model(input_seq)\n","    output = output.to(device)\n","    target_seq = target_seq.to(device)\n","    loss = criterion(output.view(-1,dict_size), target_seq.view(-1).long())\n","    loss.backward() # Does backpropagation and calculates gradients\n","    optimizer.step() # Updates the weights accordingly\n","    \n","    if epoch%10 == 0:\n","        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n","        print(\"Loss: {:.4f}\".format(loss.item()))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(model, character):\n","    # One-hot encoding our input to fit into the model\n","    character = np.array([[char2int[c] for c in character]])\n","    character = torch.from_numpy(character)\n","    character = character.to(device) \n","    out, hidden = model(character)\n","    out = out.squeeze(0)\n","    prob = nn.functional.softmax(out[-1], dim=0).data\n","    # Taking the class with the highest probability score from the output\n","    char_ind = torch.max(prob, dim=0)[1].item()\n","    print(char_ind)\n","\n","    return int2char[char_ind], hidden\n","def sample(model, out_len, start='hey'):\n","    model.eval() # eval mode\n","    start = start.lower()\n","    # First off, run through the starting characters\n","    chars = [ch for ch in start]\n","    size = out_len - len(chars)\n","    # Now pass in the previous characters and get a new one\n","    for ii in range(size):\n","        char, h = predict(model, chars)\n","        chars.append(char)\n","\n","    return ''.join(chars)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(sample(model, 15, 'hey'))\n",""]},{"cell_type":"markdown","metadata":{},"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}